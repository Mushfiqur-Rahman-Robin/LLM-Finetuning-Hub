# Data Ingestion -------------------
data:
  # type: 'json'             # one of 'json', 'csv', 'huggingface'
  # path: './examples/example_data.json'
  # type: 'huggingface'             # one of 'json', 'csv', 'huggingface'
  # path: 'yahma/alpaca-cleaned'
  prompt: >-              # prompt, make sure column inputs are enclosed in {} brackets and that they match your data
    Below is an instruction that describes a task. 
    Write a response that appropriately completes the request. 
    ### Instruction: parse the following email to the correct json output
    ### Input: {body_text} 
    ### Output: 
  prompt_stub: >-         # Stub to add for training at the end of prompt, for test set or inference, this is omitted; make sure only one variable is present
    {email_json}
  test_size: 0.2          # Proportion of test as % of total 


# Model Definition -------------------
model:
  model_ckpt: 'teknium/OpenHermes-2-Mistral-7B'
  quant_4bit: true


# LoRA Params -------------------
lora:
  task_type: "CAUSAL_LM"
  r:  64
  alpha_factor: 2
  lora_dropout: 0.1
  bias: "none"
  target_modules: 
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - lm_head

# Training -------------------
training:
  output_dir: './experiments/'
  num_train_epochs: 3
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  gradient_checkpointing: True
  # optim: "paged_adamw_32bit"
  optim: "adafactor"
  logging_steps: 100
  learning_rate: 2.0e-4
  bf16: true      # Set to true for mixed precision training on Newer GPUs
  tf32: true
  # fp16: false     # Set to true for mixed precision training on Older GPUs
  max_grad_norm: 0.3
  warmup_ratio: 0.03
  lr_scheduler_type: "constant"

sft:
  max_seq_length: 2000


inference:
  max_new_tokens: 2000
  do_sample: True
  top_p: 0.95
  temperature: 1.0e-3